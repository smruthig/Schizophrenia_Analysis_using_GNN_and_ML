{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZO0efz7FKcNH"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34Sj6CEbxsHH"},"outputs":[],"source":["#installs\n","!pip install stellargraph\n","!pip install pyyaml h5py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrtnMu3bNDm4"},"outputs":[],"source":["#imports \n","\n","from stellargraph import StellarGraph\n","import stellargraph as sg\n","from stellargraph.mapper import PaddedGraphGenerator\n","from stellargraph.layer import GCNSupervisedGraphClassification, DeepGraphCNN\n","import pandas as pd\n","import os\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import model_selection\n","from IPython.display import display, HTML\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.losses import binary_crossentropy\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.metrics import f1_score, precision_score, confusion_matrix, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"w3bnXeuXM_am"},"source":["### Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s51S4IlaPnle"},"outputs":[],"source":["combined_phenotype='/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Metadata/phenotypic_data.csv'\n","aug2_label = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Metadata/aug_labels.csv'\n","labels_mappings= pd.read_csv(combined_phenotype)\n","aug2_labels = pd.read_csv(aug2_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DazwG5AQoUn"},"outputs":[],"source":["def row_transform(arr, threshold):\n","  for i in range(len(arr)):\n","      arr[i] = arr[i] if arr[i]> threshold else abs(arr[i])+0.0001\n","  return arr\n","\n","def binarize(df, threshold):\n","    df = df.transform(lambda x: row_transform(x, threshold))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKnL6r1UxYBe"},"outputs":[],"source":["ub_02 = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Preprocessed Data/UCLA Binary 0_2'\n","uw = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Preprocessed Data/UCLA Weighted v2'\n","\n","binary_features_02 = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/UCLA Binary Local 0_2/'\n","weighted_features = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/Weighted local feature files/'\n","\n","uba = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Preprocessed Data/UCLA Augmented v1 Binary 0_2'\n","uwa = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Preprocessed Data/UCLA Augmented v1'\n","\n","weighted_features_aug = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/Local Weighted features - UCLA Aug/'\n","binary_features_aug = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/Local Binary features -  UCLA Aug/'\n","\n","uba1 = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Preprocessed Data/UCLA Augmented v2 Binary 0_2/'\n","uwa1 = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Preprocessed Data/UCLA Augmented v2/'\n","\n","weighted_features_aug1 = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/Local Weighted features- UCLA Aug v2/'\n","binary_features_aug1 = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/Local Binary features - UCLA Aug v2/'\n","\n","node2vec_features = '/content/drive/MyDrive/SUPPLEMENTARY CODE/Data/Generated Features/node2vec/'"]},{"cell_type":"code","source":["graphs_dir = [uba1, uba, ub_02]\n","feat_dir = [(binary_features_02, \"binary_node_features_ucla_binary_\"), (weighted_features, \"weighted_node_features_\")]\n","aug_feat_dir = [(binary_features_aug, \"binary_node_features_\"), (weighted_features_aug, \"weighted_node_features_\")]\n","aug_feat_dir1 = [(binary_features_aug1, \"binary_node_features_\"), (weighted_features_aug1, \"weighted_node_features_\")]"],"metadata":{"id":"aT7b-jlwqJ4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikl0M337zW3q"},"outputs":[],"source":["in_feat=0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80-tILrruzCv"},"outputs":[],"source":["graphs = list()\n","graph_labels = list()\n","\n","# Graph creation\n","for directory in graphs_dir:\n","  for file in os.listdir(directory):\n","    if '(' in file:\n","        continue\n","    if file.endswith('.csv'):\n","      if (directory == ub_02):\n","          subject = file[12:16]\n","          if int(subject) == 2155:\n","            continue\n","      elif (directory == uw):\n","          subject = file[14:18]\n","      elif (directory == uba):\n","          subject = file[12:16]\n","          if int(subject) < 3125:\n","            continue\n","      elif (directory == uba1):\n","          subject = file[12:16]\n","      else:\n","        print(\"INVALID DIRECTORY\")\n","        exit(0)\n","\n","      # APPENDING LABEL\n","      subject = int(subject)\n","      print(subject)\n","      mask = labels_mappings['Subject'] == subject\n","      #only for aug v2\n","      if directory ==  uba1:\n","        if(labels_mappings[mask]['Label'].values[0]==0 or \n","            (labels_mappings[mask]['Label'].values[0]!=aug2_labels.query(\"Subject==@subject\")[\"assumed\"].values[0])):\n","            continue\n","      graph_labels.append(labels_mappings[mask]['Label'].values[0])\n","\n","      # APPENDING CORRESPONDING GRAPH\n","      df = pd.read_csv(directory+'/'+file, header=None)\n","      df = df.fillna(0)\n","      if directory == uba:\n","        for col in df:\n","          df[col] = df[col].astype(np.int64)\n","      G = nx.from_pandas_adjacency(df)          \n","\n","      # Features\n","      if directory == uba: \n","        if len(feat_dir) == 1:\n","          node_data = pd.read_csv(aug_feat_dir[0][0]+aug_feat_dir[0][1]+str(subject)+'.csv')\n","          node_data = node_data.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","        else: \n","          node_data_1 = pd.read_csv(aug_feat_dir[0][0]+aug_feat_dir[0][1]+str(subject)+'.csv')\n","          node_data_2 = pd.read_csv(aug_feat_dir[1][0]+aug_feat_dir[1][1]+str(subject)+'.csv')\n","          node_data_1 = node_data_1.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","          node_data_2 = node_data_2.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","          node_data = pd.merge(node_data_1, node_data_2, how = \"inner\", left_index=True, right_index=True)\n","      elif directory == uba1:\n","        if len(feat_dir) == 1:\n","          node_data = pd.read_csv(aug_feat_dir1[0][0]+aug_feat_dir1[0][1]+str(subject)+'.csv')\n","          node_data = node_data.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","        else: \n","          node_data_1 = pd.read_csv(aug_feat_dir1[0][0]+aug_feat_dir1[0][1]+str(subject)+'.csv')\n","          node_data_2 = pd.read_csv(aug_feat_dir1[1][0]+aug_feat_dir1[1][1]+str(subject)+'.csv')\n","          node_data_1 = node_data_1.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","          node_data_2 = node_data_2.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","          node_data = pd.merge(node_data_1, node_data_2, how = \"inner\", left_index=True, right_index=True) \n","      else:\n","        if len(feat_dir) == 1:\n","          node_data = pd.read_csv(feat_dir[0][0]+feat_dir[0][1]+str(subject)+'.csv')\n","          node_data = node_data.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","        else:\n","          node_data_1 = pd.read_csv(feat_dir[0][0]+feat_dir[0][1]+str(subject)+'.csv')\n","          node_data_2 = pd.read_csv(feat_dir[1][0]+feat_dir[1][1]+str(subject)+'.csv')\n","          node_data_1 = node_data_1.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","          node_data_2 = node_data_2.drop(['Unnamed: 0'], axis=1, errors='ignore')\n","          node_data = pd.merge(node_data_1, node_data_2, how = \"inner\", left_index=True, right_index=True)\n","\n","      node_data = node_data.drop(['subgraph centrality'], axis=1, errors='ignore')\n","\n","      # Standardisation\n","      cols = list(node_data)\n","      scaler = StandardScaler().fit(node_data)\n","      node_data = scaler.transform(node_data)\n","      node_data = pd.DataFrame(node_data, columns = cols)\n","      \n","      g = StellarGraph.from_networkx(G,node_features=node_data)\n","      graphs.append(g)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUyZBt78mYP_"},"outputs":[],"source":["graph_labels = pd.DataFrame(graph_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmGfxhMICpMq"},"outputs":[],"source":["graph_labels = pd.get_dummies(graph_labels, drop_first=True)"]},{"cell_type":"markdown","metadata":{"id":"-CufcfzeHY82"},"source":["### GCN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05KvxfZfC0s1"},"outputs":[],"source":["generator = PaddedGraphGenerator(graphs=graphs)"]},{"cell_type":"code","source":["from sklearn import metrics\n","acc = []\n","pre = []\n","re = []\n","spe = []\n","f = []\n","es = EarlyStopping(monitor=\"val_acc\", min_delta=0, patience=30, restore_best_weights=True)\n","epochs=100\n","for rs in list(range(1,11)):\n","  print(rs)\n","  gc_model = GCNSupervisedGraphClassification(\n","        layer_sizes=[in_feat, 32, 64, 128, 64, 32, 16, 4, 2],\n","        activations=[\"leaky_relu\", \"leaky_relu\", \"leaky_relu\",  \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"leaky_relu\", \"log_softmax\"],\n","        generator=generator,\n","        pool_all_layers = True,\n","        pooling = None,\n","        dropout=0.5,\n","    )\n","  x_inp, x_out = gc_model.in_out_tensors()\n","\n","  predictions = Dense(units=16, activation=\"relu\")(x_out)\n","  #predictions = Dense(units=16, activation=\"relu\")(predictions)\n","  predictions = Dense(units=1, activation=\"sigmoid\")(predictions)\n","\n","  model = Model(inputs=x_inp, outputs=predictions)\n","  model.compile(\n","      optimizer=Adam(learning_rate=0.0005), loss=binary_crossentropy, metrics=[\"acc\"],\n","  )\n","\n","  train_graphs, test_graphs = model_selection.train_test_split(graph_labels, train_size=0.8, test_size=0.2, stratify=graph_labels, random_state=rs)\n","  gen = PaddedGraphGenerator(graphs=graphs)\n","\n","  train_gen = gen.flow(\n","      list(train_graphs.index - 1),\n","      targets=train_graphs.values,\n","      batch_size=50,\n","      symmetric_normalization=True,\n","  )\n","\n","  test_gen = gen.flow(\n","      list(test_graphs.index - 1),\n","      targets=test_graphs.values,\n","      batch_size=1,\n","      symmetric_normalization=True,\n","  )\n","\n","  history = model.fit(train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True, callbacks=[es],)\n","  test_metrics = model.evaluate(test_gen)\n","\n","  print(\"\\nTest Set Metrics:\")\n","  print(\"Validation Accuracy: \", max(history.history['val_acc']))\n","  print(\"Validation Loss: \", min(history.history['val_loss']))\n","\n","  print(\"\\nTraining Set Metrics:\")\n","  print(\"Accuracy: \", max(history.history['acc']))\n","  print(\"Loss: \", min(history.history['loss']))\n","\n","  pred = model.predict(test_gen)\n","\n","  y_pred = []\n","  y_true = []\n","  x = np.array(pred)\n","  for i in x:\n","    if i <0.5:\n","      y_pred.append(0)\n","    else:\n","      y_pred.append(1)\n","\n","  for t in test_gen.targets:\n","      y_true.append(t[0])\n","\n","  print(confusion_matrix(y_true, y_pred))\n","\n","\n","  f1_score = metrics.f1_score(y_true, y_pred)\n","  precision = metrics.precision_score(y_true, y_pred)\n","  recall =  metrics.recall_score(y_true, y_pred)\n","  accuracy = metrics.accuracy_score(y_true, y_pred)\n","\n","  print(\"Metrics:\")\n","  print(\"Accuracy: \", accuracy)\n","  print(\"f1 score: \", f1_score)\n","  print(\"precision: \", precision)\n","  print(\"recall: \", recall)\n","  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","  specificity = tn / (tn+fp)\n","  print(\"Specificity\", specificity)\n","\n","  acc.append(accuracy)\n","  f.append(f1_score)\n","  pre.append(precision)\n","  re.append(recall)\n","  spe.append(specificity)\n","\n","  print('\\n\\n')\n","\n","print('Average accccuracy: %.3f +/- %.3f' %(np.mean(acc), np.std(acc)))\n","print('Average precision: %.3f +/- %.3f' %(np.mean(pre), np.std(pre)))\n","print('Average recall: %.3f +/- %.3f' %(np.mean(re), np.std(re)))\n","print('Average f1: %.3f +/- %.3f \\n\\n' %(np.mean(f), np.std(f)))\n","print('Average specificity: %.3f +/- %.3f \\n\\n' %(np.mean(spe), np.std(spe)))"],"metadata":{"id":"bajZBv5rDQDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Average accccuracy: %.3f +/- %.3f' %(np.mean(acc), np.std(acc)))\n","print('Average precision: %.3f +/- %.3f' %(np.mean(pre), np.std(pre)))\n","print('Average recall: %.3f +/- %.3f' %(np.mean(re), np.std(re)))\n","print('Average f1: %.3f +/- %.3f \\n\\n' %(np.mean(f), np.std(f)))\n","print('Average specificity: %.3f +/- %.3f \\n\\n' %(np.mean(spe), np.std(spe)))"],"metadata":{"id":"OTCmkjKJxeI9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}